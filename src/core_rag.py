import os
import re
import requests
from datetime import datetime
from dotenv import load_dotenv

from langchain_mistralai import MistralAIEmbeddings, ChatMistralAI
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

load_dotenv()


# R√©cup√®re la cl√© (soit du .env local, soit du secret GitHub)
MISTRAL_API_KEY = os.getenv("MISTRAL_API_KEY")

if not MISTRAL_API_KEY:
    print("‚ö†Ô∏è Attention : MISTRAL_API_KEY est introuvable !")

def clean_html(text):
    """Supprime les balises HTML pour un contexte plus propre."""
    if not text: return ""
    clean = re.sub(r'<[^>]+>', '', text)
    return clean.strip()

class RAGSystem:
    def __init__(self, index_path="data/vdb_paris"):
        self.index_path = index_path
        # Utilisation du mod√®le d'embedding Mistral pour la coh√©rence vectorielle
        self.embeddings = MistralAIEmbeddings(model="mistral-embed")
        # Temp√©rature basse (0.1) pour garantir la fid√©lit√© aux documents
        self.llm = ChatMistralAI(model="mistral-small-latest", temperature=0.1)
        self.vector_db = None  # Initialize as None, load only if exists
        if os.path.exists(self.index_path):
            self.vector_db = self._load_db()

    def _load_db(self):
        """Charge l'index FAISS local de mani√®re s√©curis√©e."""
        # Check if the FAISS index file actually exists, not just the directory
        index_file = os.path.join(self.index_path, "index.faiss")
        if os.path.exists(index_file):
            return FAISS.load_local(
                self.index_path, 
                self.embeddings, 
                allow_dangerous_deserialization=True
            )
        return None

    def rebuild_database(self):
        """Action du endpoint /rebuild : Capture, Nettoyage et Indexation."""
        print("üîÑ Reconstruction de la base avec les filtres 2025-2026...")
        
        # Utilisation de l'URL et des param√®tres valid√©s en test
        url = "https://public.opendatasoft.com/api/explore/v2.1/catalog/datasets/evenements-publics-openagenda/records"
        params = {
            "limit": 100,
            "where": "lastdate_end >= date'2025' AND location_city = 'Paris'",
            "order_by": "updatedat DESC"
        }
        
        try:
            response = requests.get(url, params=params)
            response.raise_for_status()
            data = response.json().get('results', [])
        except Exception as e:
            print(f"‚ùå Erreur lors de la r√©cup√©ration API : {e}")
            return

        raw_docs = []
        for r in data:
            # Construction d'un contenu textuel riche avec labels pour faciliter la recherche
            content = (
                f"√âV√âNEMENT : {r.get('title_fr')}\n"
                f"LIEU : {r.get('location_name')} ({r.get('location_city')})\n"
                f"DATE : {r.get('firstdate_begin')}\n"
                f"DESCRIPTION : {clean_html(r.get('description_fr'))}"
            )
            raw_docs.append(Document(
                page_content=content,
                metadata={
                    "date": r.get('firstdate_begin'), 
                    "titre": r.get('title_fr')
                }
            ))

        # Chunking optimis√© pour conserver l'unit√© d'un √©v√©nement
        splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
        docs = splitter.split_documents(raw_docs)

        # Cr√©ation et sauvegarde locale
        self.vector_db = FAISS.from_documents(docs, self.embeddings)
        self.vector_db.save_local(self.index_path)
        print(f"‚úÖ Base de donn√©es vectorielle pr√™te : {len(docs)} chunks index√©s.")

    def _get_relevant_docs(self, query):
        """R√©cup√©ration s√©mantique des documents les plus proches."""
        if not self.vector_db:
            return ""
        
        # On r√©cup√®re les 5 meilleurs r√©sultats pour laisser un choix au LLM
        docs = self.vector_db.similarity_search(query, k=5)
        
        # On passe le texte brut au prompt sans filtre de date Python 
        # pour laisser l'IA g√©rer la logique temporelle.
        return "\n\n".join([d.page_content for d in docs])

    def ask(self, question: str):
        """Action du endpoint /ask avec Prompt System final."""
        if not self.vector_db:
            return "La base de donn√©es est vide. Veuillez appeler /rebuild d'abord."

        template = """Tu es l'Assistant Officiel des √âv√©nements de Paris. Ton r√¥le est d'aider les utilisateurs √† trouver des activit√©s culturelles en utilisant EXCLUSIVEMENT les informations fournies dans le contexte ci-dessous.

### R√àGLES DE CONDUITE :
1. **Priorit√© au Contexte** : Si l'information n'est pas dans le contexte ou si aucun √©v√©nement ne correspond √† la date demand√©e, r√©ponds que tu ne trouves pas d'information correspondante.
2. **Format de R√©ponse** : 
   - **Titre de l'√©v√©nement** (en gras)
   - üìç Lieu : [Nom du lieu]
   - üìÖ Date : [Date format√©e]
   - üìù R√©sum√© : [2-3 phrases max sur l'int√©r√™t de l'√©v√©nement]
3. **Intelligence Temporelle** : Utilise la "DATE ACTUELLE" pour √©valuer si un √©v√©nement est pass√© ou futur.
4. **Ton** : Professionnel, clair et accueillant.

### CONTEXTE :
{context}

### DATE ACTUELLE :
{current_date}

QUESTION DE L'UTILISATEUR : {question}

R√âPONSE :"""
        
        prompt = ChatPromptTemplate.from_template(template)
        
        # Cha√Æne de traitement LangChain (LCEL)
        chain = (
            {
                "context": lambda x: self._get_relevant_docs(x), 
                "question": RunnablePassthrough(),
                "current_date": lambda _: datetime.now().strftime('%d/%m/%Y')
            }
            | prompt | self.llm | StrOutputParser()
        )
        return chain.invoke(question)